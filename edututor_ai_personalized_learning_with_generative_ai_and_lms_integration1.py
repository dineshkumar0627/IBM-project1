# -*- coding: utf-8 -*-
"""EduTutor AI: Personalized Learning with Generative AI and LMS Integration1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14mJaF93h7JJPEfK3ekoAh5c_0RJYDRsf
"""

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ------------------------
# Model Configuration
# ------------------------
model_name = "ibm-granite/granite-3.2-2b-instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set pad_token if missing
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

# Load model with proper device mapping
if torch.cuda.is_available():
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32
    )

# Resize embeddings
model.resize_token_embeddings(len(tokenizer))


# ------------------------
# Helper Functions
# ------------------------
def generate_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512, padding=True)

    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id
        )

    response = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

    # Remove prompt if repeated
    if response.startswith(prompt):
        response = response[len(prompt):].strip()

    return response


def concept_explanation(concept):
    prompt = f"Explain the concept of {concept} in detail with examples:"
    return generate_response(prompt, max_length=800)


def quiz_generator(concept):
    prompt = (
        f"Generate 5 quiz questions about {concept} with different question types "
        f"(multiple choice, true/false, short answer). "
        f"At the end, provide all the answers in a separate ANSWERS section:"
    )
    return generate_response(prompt, max_length=1000)


# ------------------------
# Gradio Interface
# ------------------------
with gr.Blocks() as app:
    gr.Markdown("# üìò Educational AI Assistant")
    gr.Markdown("Ask the AI to explain a concept or generate quiz questions on any topic!")

    with gr.Tabs():
        with gr.Tab("üìö Concept Explanation"):
            concept_input = gr.Textbox(label="Enter a concept", placeholder="e.g., machine learning")
            explain_btn = gr.Button("Explain")
            explanation_output = gr.Textbox(label="Explanation", lines=10)
            explain_btn.click(concept_explanation, inputs=concept_input, outputs=explanation_output)

        with gr.Tab("üìù Quiz Generator"):
            quiz_input = gr.Textbox(label="Enter a topic", placeholder="e.g., physics")
            quiz_btn = gr.Button("Generate Quiz")
            quiz_output = gr.Textbox(label="Quiz Questions", lines=15)
            quiz_btn.click(quiz_generator, inputs=quiz_input, outputs=quiz_output)

# Launch the app
app.launch(share=True)